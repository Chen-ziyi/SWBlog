之前简单介绍了决策树，这篇文章简单介绍一下随机森林以及优缺点。
### 集成学习
通过构建并结合多个分类器来完成学习任务。将多个学习器进行结合，常比获得单一学习器更好的泛化性能。
目前集成学习方法大致可分为两类，即个体学习器之间存在强依赖关系，必须串行生成的序列化方法，以及个体学习器之间不存在依赖关系，可同时生成的并行化方法；前者代表时Boosting， 后者代表是Bagging和随机森林(random forest: RF)。

#### Bagging 和随机森林
要得到泛化性能强的集成，则集成中的个体学习器应尽可能相互独立。
“自助采样法”：给定包含m个样本的数据集， 先随机选取一个样本放入采样集中，再把该样本放回，重复m次随机操作，得到含m个样本的采样集。这样使得初始训练集中有的样本在采样集中出现，有的从未出现。
如此，可以采样出T个含m个样本的采样集，基于每个采样集训练出一个基础学习器，将这些基础学习器进行结合，这就是Bagging的基本流程。在对预测输出进行结合时， Bagging通常对分类任务使用简单投票，对回归任务使用简单平均法。

关于决策树：
决策树实际是将空间用超平面(后面介绍svm也会提到)进行划分的一种方法，每次分割，都将当前空间一份为二。  
![image.png](https://user-gold-cdn.xitu.io/2018/3/2/161e67537915cc9a?w=620&h=484&f=png&s=61906)  
这样使得每一个叶子节点都是在空间中的一个不相交的区域。决策时，会根据输入样本每一维feature的值，一步步往下，最后使样本落入N个区域中的一个(假设有N个叶子节点)。

随机森林是Bagging的一个扩展。**随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择(引入随机特征选择)**。传统决策树在选择划分属性时在当前节点的属性结合(d个属性)，利用信息论的知识选取一个最优属性；而在随机森林中， 对决策树的每个节点，先从该节点的属性集合中随机选取包含k个属性的子属性集，然后选择最优属性用于划分。这里的参数k控制了随机性的引入程度。若k=d， 则是一般的决策树；k=1， 则是随机选择一个属性进行划分。随机森林对用做构建树的数据做了限制，使的生成的决策树之间没有关联，提升算法效果。

#### 随机森林的分类
随机森林用于分类是，即采用n个决策树分类，将分类结果用简单投票得到最终分类，提高准确率。
随机森林是对决策树的集成，其两点不同也在上面叙述中提到：
1. 采样差异：从含m个样本的数据集中**有放回**采样，得到含m个样本的采样集用于训练。保证每个决策树的训练样本不完全一样。
2. 特征选择差异：每个决策树的k个分类特征是所在特征中随机选择的(k需要调参)。

随机森林需要调整的参数：
* 决策树的个数m
* 特征属性的个数 k
* 递归次数(决策树的深度)

### 实现流程
1. 导入数据并将特征转为`float`形式。
2. 将数据集分成n份， 方便交叉验证
3. 构造数据子集(随机采样)，并在指定特征个数(假设m个，调参)下选择最优特征
4. 构造决策树(决策树的深度)
5. 创建随机森林(多个决策树的结合)
6. 输入测试集并进行测试，输入预测结果。

随机森林的优点：
* 在当前的很多数据集上，相对其他算法有着很大的优势
* 能够处理高纬度(feature)的数据， 并且不同做特征选择。
* 在训练完后，能够给出哪些feature比较重要
* 创建随机森林时， 对generlization error使用的是无偏估计
* 训练速度快
* 在训练过程中，能够监测到feature间的相互影响
* 容易做成并行化方法
* 理解，实现简单

基本理解后，可以参考一下别人和sklearn的相关算法实现，可能的话，我也会做个简单实现。

参考文章：  
 [机器学习中的算法(1)-决策树模型组合之随机森林与GBDT](http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html)  
[随机森林的原理分析及Python代码实现](http://blog.csdn.net/Flying_sfeng/article/details/64133822)  




